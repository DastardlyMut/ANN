It seems that there a certain 'sweet spot' that allows for the network to train correctly.

Putting 20 neurons in both layers results with an R^2 value of 0.5827 which shows that there is no correlation
between the neural network's activation and the targets and the net has not been trained correctly.

When putting 1 neuron in the first layer and 100 in the second layer, the net does NOT train correctly.

However, putting 100 neurons in the first layer and 1 neuron in the second, the net trains correctly. 

The performance difference between having 100 neurons in the first layer or 1 neuron in the first layer is
very small and both train properly.

This shows that for this problem, it is likely unnecessary to have the second layer since the net
can train correctly with a sufficiently small second layer (best if second layer is under 9 neurons). 
But the first layer can have many or few neurons and the net still trains correctly.
